{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2-11B Vision NER Package Demo\n",
    "\n",
    "This notebook demonstrates the Llama 3.2-11B Vision model functionality using InternVL PoC architecture patterns.\n",
    "\n",
    "**KEY-VALUE extraction is the primary and preferred method** - JSON extraction is legacy and less reliable.\n",
    "\n",
    "Following the hybrid approach: **InternVL PoC's superior architecture + Llama-3.2-11B-Vision model**\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "**Required**: Use the `internvl_env` conda environment:\n",
    "\n",
    "```bash\n",
    "# Activate the conda environment\n",
    "conda activate internvl_env\n",
    "\n",
    "# Launch Jupyter\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "This notebook is designed to work with the same environment as the InternVL PoC for consistency and shared dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport time\nimport platform\nimport os\nfrom pathlib import Path\nimport torch\nfrom typing import Dict, Any, List\nimport json\nimport gc\n\nprint(\"üîß ENVIRONMENT VERIFICATION\")\nprint(\"=\" * 30)\nprint(f\"üì¶ Using conda environment: llama_vision_env\")\nprint(f\"üêç Python version: {platform.python_version()}\")\nprint(f\"üî• PyTorch version: {torch.__version__}\")\nprint(f\"üíª Platform: {platform.platform()}\")\n\n# V100 Optimization: Enable TF32 for faster matrix operations\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    print(\"‚ö° TF32 enabled for V100 optimization\")\n\n# Load environment variables from .env file (from current directory)\ntry:\n    from dotenv import load_dotenv\n    \n    # Load .env file from current directory (not parent)\n    env_path = Path('.env')  # Look in current directory\n    if env_path.exists():\n        load_dotenv(env_path)\n        print(f\"‚úÖ Loaded .env from: {env_path.absolute()}\")\n    else:\n        raise FileNotFoundError(f\"‚ùå No .env file found at: {env_path.absolute()}\")\n        \nexcept ImportError:\n    raise ImportError(\"‚ùå python-dotenv not installed. Install with: pip install python-dotenv\")\n\n# Environment-driven configuration (NO hardcoded defaults)\ndef load_llama_config() -> Dict[str, Any]:\n    \"\"\"Load configuration from environment variables (.env file).\"\"\"\n    \n    # ALL values must come from environment\n    required_vars = [\n        'TAX_INVOICE_NER_BASE_PATH',\n        'TAX_INVOICE_NER_MODEL_PATH'\n    ]\n    \n    # Check required variables exist\n    missing_vars = [var for var in required_vars if not os.getenv(var)]\n    if missing_vars:\n        raise ValueError(f\"‚ùå Missing required environment variables: {missing_vars}\")\n    \n    # Load from environment (no fallbacks)\n    base_path = os.getenv('TAX_INVOICE_NER_BASE_PATH')\n    model_path = os.getenv('TAX_INVOICE_NER_MODEL_PATH')\n    \n    config = {\n        'base_path': base_path,\n        'model_path': model_path,\n        'image_folder_path': os.getenv('TAX_INVOICE_NER_IMAGE_PATH', f\"{base_path}/datasets/test_images\"),\n        'output_path': os.getenv('TAX_INVOICE_NER_OUTPUT_PATH', f\"{base_path}/output\"),\n        'config_path': os.getenv('TAX_INVOICE_NER_CONFIG_PATH', f\"{base_path}/config/extractor/work_expense_ner_config.yaml\"),\n        'max_tokens': int(os.getenv('TAX_INVOICE_NER_MAX_TOKENS', '1024')),\n        'temperature': float(os.getenv('TAX_INVOICE_NER_TEMPERATURE', '0.1')),\n        'do_sample': os.getenv('TAX_INVOICE_NER_DO_SAMPLE', 'false').lower() == 'true',\n        'device': os.getenv('TAX_INVOICE_NER_DEVICE', 'auto'),\n        'use_8bit': False  # FORCE DISABLED - no bitsandbytes\n    }\n    \n    print(f\"üìã Configuration loaded from environment:\")\n    print(f\"   Base path: {config['base_path']}\")\n    print(f\"   Model path: {config['model_path']}\")\n    \n    return config\n\n# Load configuration FIRST\nconfig = load_llama_config()\n\n# THEN do device detection (after .env is loaded)\ndef auto_detect_device_config():\n    # Check for explicit device override from .env\n    env_device = config.get('device', 'auto').lower().strip()\n    \n    print(f\"üîç Device detection: env_device='{env_device}'\")\n    \n    if env_device == 'cpu':\n        return \"cpu\", 0, False\n    elif env_device == 'mps' and torch.backends.mps.is_available():\n        return \"mps\", 1, False\n    elif env_device == 'cuda' and torch.cuda.is_available():\n        num_gpus = torch.cuda.device_count()\n        return \"cuda\", num_gpus, num_gpus == 1\n    elif env_device == 'auto':\n        # Auto-detect (original logic)\n        if torch.cuda.is_available():\n            num_gpus = torch.cuda.device_count()\n            print(f\"üîç CUDA detected: {num_gpus} GPUs available\")\n            return \"cuda\", num_gpus, num_gpus == 1\n        elif torch.backends.mps.is_available():\n            print(f\"üîç MPS detected\")\n            return \"mps\", 1, False\n        else:\n            print(f\"üîç Falling back to CPU\")\n            return \"cpu\", 0, False\n    else:\n        print(f\"‚ö†Ô∏è  Unknown device '{env_device}', falling back to CPU\")\n        return \"cpu\", 0, False\n\n# Environment detection - check for model availability\nmodel_path = Path(config['model_path'])\nis_local = platform.processor() == 'arm'  # Mac M1 detection\nhas_local_model = model_path.exists()\n\nprint(\"\\nüéØ LLAMA 3.2-11B VISION NER CONFIGURATION\")\nprint(\"=\" * 45)\nprint(f\"üñ•Ô∏è  Environment: {'Local (Mac M1)' if is_local else 'Remote (Multi-GPU)'}\")\nprint(f\"üìÇ Base path: {config.get('base_path')}\")\nprint(f\"ü§ñ Model path: {config.get('model_path')}\")\nprint(f\"üìÅ Image folder: {config.get('image_folder_path')}\")\nprint(f\"‚öôÔ∏è  Config file: {config.get('config_path')}\")\nprint(f\"üîç Local model available: {'‚úÖ Yes' if has_local_model else '‚ùå No'}\")\n\n# Device detection AFTER config is loaded\ndevice_type, num_devices, use_quantization = auto_detect_device_config()\nprint(f\"üì± Device: {device_type} ({'multi-GPU' if num_devices > 1 else 'single'})\")\nprint(f\"üîß Quantization: {'Enabled' if use_quantization else 'Disabled'}\")\nprint(f\"üéõÔ∏è  Device source: {'Environment (.env)' if config.get('device') != 'auto' else 'Auto-detected'}\")\n\n# Detect GPU memory capacity for single GPU optimization\nsingle_gpu_memory = None\nif device_type == \"cuda\" and num_devices == 1:\n    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    single_gpu_memory = gpu_memory_gb\n    print(f\"üíæ Single GPU detected: {gpu_memory_gb:.1f}GB VRAM\")\n    \n    # V100 detection\n    gpu_name = torch.cuda.get_device_name(0)\n    if \"V100\" in gpu_name:\n        print(f\"üéØ V100 GPU detected: {gpu_name}\")\n        print(f\"‚ö° V100 optimizations will be applied\")\n    \n    if gpu_memory_gb < 20:\n        print(f\"‚ö†Ô∏è  GPU has {gpu_memory_gb:.1f}GB < 20GB required - will use CPU offloading\")\n    else:\n        print(f\"‚úÖ GPU has sufficient memory ({gpu_memory_gb:.1f}GB) for full model\")\nelif device_type == \"cuda\" and num_devices > 1:\n    print(f\"üíæ Multi-GPU: ~10GB per GPU with balanced splitting\")\nelse:\n    print(f\"üíæ Using CPU/MPS memory management\")\n\n# COMPREHENSIVE GPU MEMORY FLUSH\nprint(\"\\nüßπ COMPREHENSIVE GPU MEMORY FLUSH\")\nprint(\"=\" * 35)\n\n# Check current GPU memory usage BEFORE cleanup\nif torch.cuda.is_available():\n    print(f\"üîç GPU memory BEFORE cleanup:\")\n    for i in range(torch.cuda.device_count()):\n        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n        memory_reserved = torch.cuda.memory_reserved(i) / 1e9\n        print(f\"   GPU {i}: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n\n# Step 1: Delete existing model variables\nvariables_to_delete = ['model', 'processor', 'tokenizer', 'generation_config', 'model_info']\ndeleted_vars = []\n\nfor var_name in variables_to_delete:\n    if var_name in globals():\n        print(f\"   üóëÔ∏è  Deleting {var_name}\")\n        del globals()[var_name]\n        deleted_vars.append(var_name)\n\nif deleted_vars:\n    print(f\"   ‚úÖ Deleted variables: {deleted_vars}\")\nelse:\n    print(\"   ‚ÑπÔ∏è  No existing model variables found\")\n\n# Step 2: Python garbage collection\nprint(\"   üîÑ Running Python garbage collection...\")\ncollected = gc.collect()\nprint(f\"   ‚ôªÔ∏è  Collected {collected} objects\")\n\n# Step 3: PyTorch CUDA cache cleanup\nif torch.cuda.is_available():\n    print(\"   üßΩ Emptying PyTorch CUDA cache...\")\n    torch.cuda.empty_cache()\n    \n    # Step 4: Force synchronization and additional cleanup\n    print(\"   ‚è≥ Synchronizing CUDA devices...\")\n    for i in range(torch.cuda.device_count()):\n        torch.cuda.synchronize(device=f'cuda:{i}')\n    \n    # Additional aggressive cleanup\n    print(\"   üî• Aggressive memory cleanup...\")\n    torch.cuda.ipc_collect()\n    torch.cuda.empty_cache()  # Second pass\n    \n    # V100 optimization: Set memory fraction\n    if single_gpu_memory and single_gpu_memory < 20:\n        torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n        print(\"   üíæ V100: Set memory fraction to 95%\")\n    \n    print(\"   ‚úÖ GPU memory cleanup completed\")\n\n# Check GPU memory usage AFTER cleanup\nif torch.cuda.is_available():\n    print(f\"\\nüîç GPU memory AFTER cleanup:\")\n    total_freed = 0\n    for i in range(torch.cuda.device_count()):\n        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n        memory_reserved = torch.cuda.memory_reserved(i) / 1e9\n        print(f\"   GPU {i}: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n        total_freed += memory_reserved\n    \n    print(f\"   üíæ Total GPU memory available for new model: ~{total_freed:.1f}GB\")\n\n# Helper function to calculate model size\ndef get_model_size_info(model) -> Dict[str, Any]:\n    \"\"\"Calculate model size information with accurate dtype handling.\"\"\"\n    try:\n        # Count parameters\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        \n        # Get actual model dtype and calculate size accurately\n        model_dtype = next(model.parameters()).dtype\n        bytes_per_param = 2 if model_dtype == torch.float16 else 4  # fp16 = 2 bytes, fp32 = 4 bytes\n        \n        # Calculate size in bytes and GB\n        size_bytes = total_params * bytes_per_param\n        size_gb = size_bytes / (1024**3)\n        \n        return {\n            \"status\": \"loaded\",\n            \"total_params\": total_params,\n            \"trainable_params\": trainable_params,\n            \"dtype\": model_dtype,\n            \"precision_name\": \"fp16 (half)\" if model_dtype == torch.float16 else \"fp32 (float)\",\n            \"bytes_per_param\": bytes_per_param,\n            \"size_gb\": size_gb,\n            \"size_formatted\": f\"{size_gb:.2f}GB\",\n            \"params_formatted\": f\"{total_params/1e9:.1f}B parameters\"\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n# Model loading logic - FAIL if no model found\nif not has_local_model:\n    raise FileNotFoundError(f\"‚ùå Model not found at: {config['model_path']}\")\n\nprint(\"\\nüöÄ MODEL LOADING:\")\nprint(\"   - Loading Llama-3.2-11B-Vision from local path\")\nprint(f\"   - Using {device_type.upper()} for inference\")\nprint(\"   - Model requires significant memory (11B parameters)\")\n\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\n# Check for required packages\nprint(\"\\nüì¶ Package verification:\")\ntry:\n    import accelerate\n    print(f\"   ‚úÖ accelerate {accelerate.__version__}\")\nexcept ImportError:\n    print(\"   ‚ùå accelerate not installed - required for device mapping\")\n\ntry:\n    import safetensors\n    print(f\"   ‚úÖ safetensors {safetensors.__version__}\")\nexcept ImportError:\n    print(\"   ‚ö†Ô∏è  safetensors not installed - slower model loading\")\n\ntry:\n    import bitsandbytes\n    print(f\"   ‚úÖ bitsandbytes {bitsandbytes.__version__} - ready for future 8-bit quantization\")\nexcept ImportError:\n    print(\"   ‚ÑπÔ∏è  bitsandbytes not installed - 8-bit quantization unavailable\")\n\nprint(\"\\n‚è≥ Loading Llama-3.2-11B-Vision model...\")\n\nif device_type == \"cuda\":\n    model_dtype = torch.float16  # fp16 for GPU\n    print(f\"   üîß Requesting dtype: {model_dtype} (fp16 - GPU optimized)\")\n    \n    if num_devices > 1:\n        # Multi-GPU: Balanced splitting\n        print(f\"   üîß Using balanced GPU splitting across {num_devices} GPUs\")\n        \n        # Get available memory per GPU (reserve 4GB for safety)\n        gpu_memory = {}\n        for i in range(num_devices):\n            total_memory = torch.cuda.get_device_properties(i).total_memory\n            available_memory = total_memory - (4 * 1024**3)  # Reserve 4GB\n            gpu_memory[i] = f\"{available_memory // (1024**3)}GB\"\n        \n        print(f\"   üíæ Available memory per GPU: {gpu_memory}\")\n        \n        # Load model with balanced device map\n        model = MllamaForConditionalGeneration.from_pretrained(\n            config['model_path'],\n            torch_dtype=model_dtype,\n            device_map=\"balanced\",\n            max_memory=gpu_memory\n        )\n        \n        print(f\"   ‚úÖ Model split across {num_devices} GPUs with balanced memory usage\")\n        \n    else:\n        # Single GPU: Check if CPU offloading is needed\n        if single_gpu_memory and single_gpu_memory < 20:\n            # V100 16GB case - use CPU offloading\n            print(f\"   üîß Single GPU with {single_gpu_memory:.1f}GB - using CPU offloading\")\n            print(\"   üíæ Strategy: GPU for inference layers + CPU for storage layers\")\n            \n            # Reserve 2GB for CUDA overhead, use remaining for GPU\n            gpu_memory_gb = int(single_gpu_memory - 2)\n            \n            max_memory = {\n                0: f\"{gpu_memory_gb}GB\",  # Use most of GPU memory\n                \"cpu\": \"20GB\"  # Offload overflow to CPU\n            }\n            \n            print(f\"   üéØ Memory allocation: GPU={gpu_memory_gb}GB, CPU=20GB overflow\")\n            \n            # Create offload folder if needed\n            offload_folder = Path(\"./offload_cache\")\n            offload_folder.mkdir(exist_ok=True)\n            \n            # Load with CPU offloading\n            model = MllamaForConditionalGeneration.from_pretrained(\n                config['model_path'],\n                torch_dtype=model_dtype,\n                device_map=\"auto\",  # Auto-map with memory constraints\n                max_memory=max_memory,\n                offload_folder=str(offload_folder),\n                offload_state_dict=True\n            )\n            \n            print(f\"   ‚úÖ Model loaded with CPU offloading for V100 16GB compatibility\")\n            \n        else:\n            # Standard single GPU loading (>20GB VRAM)\n            device_map = \"cuda:0\"\n            print(f\"   üéØ Single GPU with sufficient memory - device map: {device_map}\")\n            \n            model = MllamaForConditionalGeneration.from_pretrained(\n                config['model_path'],\n                torch_dtype=model_dtype,\n                device_map=device_map\n            )\n            \n            print(f\"   ‚úÖ Model loaded entirely on GPU 0\")\n        \nelse:\n    model_dtype = torch.float32  # Keep fp32 for CPU compatibility\n    print(f\"   üîß Requesting dtype: {model_dtype} (fp32 - CPU compatible)\")\n    device_map = \"cpu\"\n    print(f\"   üéØ Device map: {device_map}\")\n    \n    # Load model on CPU\n    model = MllamaForConditionalGeneration.from_pretrained(\n        config['model_path'],\n        torch_dtype=model_dtype,\n        device_map=device_map\n    )\n\nprocessor = AutoProcessor.from_pretrained(config['model_path'])\ntokenizer = processor.tokenizer\n\ngeneration_config = {\n    \"max_new_tokens\": config.get('max_tokens', 1024),\n    \"do_sample\": config.get('do_sample', False),\n    \"temperature\": config.get('temperature', 0.1)\n}\n\n# Get model size information\nmodel_info = get_model_size_info(model)\n\nprint(\"‚úÖ Llama-3.2-11B-Vision model loaded successfully!\")\nprint(f\"   üì± Device: {model.device if hasattr(model, 'device') else 'Multiple devices'}\")\nprint(f\"   üéØ Actual dtype: {model.dtype}\")\n\n# Display model size information\nif model_info[\"status\"] == \"loaded\":\n    print(f\"   üî¢ Precision: {model_info['precision_name']} ({model_info['bytes_per_param']} bytes/param)\")\n    print(f\"   üìè Model size: {model_info['size_formatted']} ({model_info['params_formatted']})\")\n    print(f\"   üî¢ Total parameters: {model_info['total_params']:,}\")\n    print(f\"   üéØ Trainable parameters: {model_info['trainable_params']:,}\")\n\n# Display memory usage per GPU\nif device_type == \"cuda\":\n    print(f\"   üß† Memory usage per GPU AFTER loading:\")\n    for i in range(num_devices):\n        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n        print(f\"      GPU {i}: {memory_allocated:.1f}GB\")\n    \n    if num_devices > 1:\n        total_memory = sum(torch.cuda.memory_allocated(i) / 1e9 for i in range(num_devices))\n        print(f\"   üìä Total memory used: {total_memory:.1f}GB across {num_devices} GPUs\")\n        print(f\"   ‚ö° Note: Model split for balanced memory usage\")\n    elif single_gpu_memory and single_gpu_memory < 20:\n        print(f\"   üíæ V100 16GB mode: GPU handles inference, CPU stores overflow layers\")\n        print(f\"   ‚ö° Note: Slight performance penalty for CPU offloading, but fits in 16GB\")\n        print(f\"   üí° Tip: Future 8-bit quantization will eliminate need for CPU offloading\")\n    else:\n        print(f\"   ‚ö° Note: GPU inference will be much faster\")\nelse:\n    print(f\"   üß† Memory: Managed by CPU\")\n    print(f\"   ‚ö†Ô∏è  Note: CPU inference will be slower than GPU\")\n\nprint(f\"\\nüìä Configuration Summary:\")\nfor key, value in config.items():\n    if isinstance(value, (str, int, float, bool)):\n        print(f\"   {key}: {value}\")\n\nprint(\"\\n‚úÖ Package configuration completed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import platform\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Dict, Any, List\n",
    "import json\n",
    "import gc\n",
    "\n",
    "print(\"üîß ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üì¶ Using conda environment: llama_vision_env\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª Platform: {platform.platform()}\")\n",
    "\n",
    "# Load environment variables from .env file (from current directory)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load .env file from current directory (not parent)\n",
    "    env_path = Path('.env')  # Look in current directory\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"‚úÖ Loaded .env from: {env_path.absolute()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"‚ùå No .env file found at: {env_path.absolute()}\")\n",
    "        \n",
    "except ImportError:\n",
    "    raise ImportError(\"‚ùå python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# Environment-driven configuration (NO hardcoded defaults)\n",
    "def load_llama_config() -> Dict[str, Any]:\n",
    "    \"\"\"Load configuration from environment variables (.env file).\"\"\"\n",
    "    \n",
    "    # ALL values must come from environment\n",
    "    required_vars = [\n",
    "        'TAX_INVOICE_NER_BASE_PATH',\n",
    "        'TAX_INVOICE_NER_MODEL_PATH'\n",
    "    ]\n",
    "    \n",
    "    # Check required variables exist\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"‚ùå Missing required environment variables: {missing_vars}\")\n",
    "    \n",
    "    # Load from environment (no fallbacks)\n",
    "    base_path = os.getenv('TAX_INVOICE_NER_BASE_PATH')\n",
    "    model_path = os.getenv('TAX_INVOICE_NER_MODEL_PATH')\n",
    "    \n",
    "    config = {\n",
    "        'base_path': base_path,\n",
    "        'model_path': model_path,\n",
    "        'image_folder_path': os.getenv('TAX_INVOICE_NER_IMAGE_PATH', f\"{base_path}/datasets/test_images\"),\n",
    "        'output_path': os.getenv('TAX_INVOICE_NER_OUTPUT_PATH', f\"{base_path}/output\"),\n",
    "        'config_path': os.getenv('TAX_INVOICE_NER_CONFIG_PATH', f\"{base_path}/config/extractor/work_expense_ner_config.yaml\"),\n",
    "        'max_tokens': int(os.getenv('TAX_INVOICE_NER_MAX_TOKENS', '1024')),\n",
    "        'temperature': float(os.getenv('TAX_INVOICE_NER_TEMPERATURE', '0.1')),\n",
    "        'do_sample': os.getenv('TAX_INVOICE_NER_DO_SAMPLE', 'false').lower() == 'true',\n",
    "        'device': os.getenv('TAX_INVOICE_NER_DEVICE', 'auto'),\n",
    "        'use_8bit': False  # FORCE DISABLED - no bitsandbytes\n",
    "    }\n",
    "    \n",
    "    print(f\"üìã Configuration loaded from environment:\")\n",
    "    print(f\"   Base path: {config['base_path']}\")\n",
    "    print(f\"   Model path: {config['model_path']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration FIRST\n",
    "config = load_llama_config()\n",
    "\n",
    "# THEN do device detection (after .env is loaded)\n",
    "def auto_detect_device_config():\n",
    "    # Check for explicit device override from .env\n",
    "    env_device = config.get('device', 'auto').lower().strip()\n",
    "    \n",
    "    print(f\"üîç Device detection: env_device='{env_device}'\")\n",
    "    \n",
    "    if env_device == 'cpu':\n",
    "        return \"cpu\", 0, False\n",
    "    elif env_device == 'mps' and torch.backends.mps.is_available():\n",
    "        return \"mps\", 1, False\n",
    "    elif env_device == 'cuda' and torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        return \"cuda\", num_gpus, num_gpus == 1\n",
    "    elif env_device == 'auto':\n",
    "        # Auto-detect (original logic)\n",
    "        if torch.cuda.is_available():\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            print(f\"üîç CUDA detected: {num_gpus} GPUs available\")\n",
    "            return \"cuda\", num_gpus, num_gpus == 1\n",
    "        elif torch.backends.mps.is_available():\n",
    "            print(f\"üîç MPS detected\")\n",
    "            return \"mps\", 1, False\n",
    "        else:\n",
    "            print(f\"üîç Falling back to CPU\")\n",
    "            return \"cpu\", 0, False\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Unknown device '{env_device}', falling back to CPU\")\n",
    "        return \"cpu\", 0, False\n",
    "\n",
    "# Environment detection - check for model availability\n",
    "model_path = Path(config['model_path'])\n",
    "is_local = platform.processor() == 'arm'  # Mac M1 detection\n",
    "has_local_model = model_path.exists()\n",
    "\n",
    "print(\"\\nüéØ LLAMA 3.2-11B VISION NER CONFIGURATION\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üñ•Ô∏è  Environment: {'Local (Mac M1)' if is_local else 'Remote (Multi-GPU)'}\")\n",
    "print(f\"üìÇ Base path: {config.get('base_path')}\")\n",
    "print(f\"ü§ñ Model path: {config.get('model_path')}\")\n",
    "print(f\"üìÅ Image folder: {config.get('image_folder_path')}\")\n",
    "print(f\"‚öôÔ∏è  Config file: {config.get('config_path')}\")\n",
    "print(f\"üîç Local model available: {'‚úÖ Yes' if has_local_model else '‚ùå No'}\")\n",
    "\n",
    "# Device detection AFTER config is loaded\n",
    "device_type, num_devices, use_quantization = auto_detect_device_config()\n",
    "print(f\"üì± Device: {device_type} ({'multi-GPU' if num_devices > 1 else 'single'})\")\n",
    "print(f\"üîß Quantization: {'Enabled' if use_quantization else 'Disabled'}\")\n",
    "print(f\"üéõÔ∏è  Device source: {'Environment (.env)' if config.get('device') != 'auto' else 'Auto-detected'}\")\n",
    "print(f\"üíæ 8-bit quantization: ‚ùå Disabled (using fp16 - ~10GB per GPU)\")\n",
    "\n",
    "# COMPREHENSIVE GPU MEMORY FLUSH\n",
    "print(\"\\nüßπ COMPREHENSIVE GPU MEMORY FLUSH\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check current GPU memory usage BEFORE cleanup\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîç GPU memory BEFORE cleanup:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        print(f\"   GPU {i}: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n",
    "\n",
    "# Step 1: Delete existing model variables\n",
    "variables_to_delete = ['model', 'processor', 'tokenizer', 'generation_config', 'model_info']\n",
    "deleted_vars = []\n",
    "\n",
    "for var_name in variables_to_delete:\n",
    "    if var_name in globals():\n",
    "        print(f\"   üóëÔ∏è  Deleting {var_name}\")\n",
    "        del globals()[var_name]\n",
    "        deleted_vars.append(var_name)\n",
    "\n",
    "if deleted_vars:\n",
    "    print(f\"   ‚úÖ Deleted variables: {deleted_vars}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è  No existing model variables found\")\n",
    "\n",
    "# Step 2: Python garbage collection\n",
    "print(\"   üîÑ Running Python garbage collection...\")\n",
    "collected = gc.collect()\n",
    "print(f\"   ‚ôªÔ∏è  Collected {collected} objects\")\n",
    "\n",
    "# Step 3: PyTorch CUDA cache cleanup\n",
    "if torch.cuda.is_available():\n",
    "    print(\"   üßΩ Emptying PyTorch CUDA cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Step 4: Force synchronization and additional cleanup\n",
    "    print(\"   ‚è≥ Synchronizing CUDA devices...\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.synchronize(device=f'cuda:{i}')\n",
    "    \n",
    "    # Additional aggressive cleanup\n",
    "    print(\"   üî• Aggressive memory cleanup...\")\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()  # Second pass\n",
    "    \n",
    "    print(\"   ‚úÖ GPU memory cleanup completed\")\n",
    "\n",
    "# Check GPU memory usage AFTER cleanup\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüîç GPU memory AFTER cleanup:\")\n",
    "    total_freed = 0\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        print(f\"   GPU {i}: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n",
    "        total_freed += memory_reserved\n",
    "    \n",
    "    print(f\"   üíæ Total GPU memory available for new model: ~{total_freed:.1f}GB\")\n",
    "\n",
    "# Helper function to calculate model size\n",
    "def get_model_size_info(model) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate model size information with accurate dtype handling.\"\"\"\n",
    "    try:\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Get actual model dtype and calculate size accurately\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "        bytes_per_param = 2 if model_dtype == torch.float16 else 4  # fp16 = 2 bytes, fp32 = 4 bytes\n",
    "        \n",
    "        # Calculate size in bytes and GB\n",
    "        size_bytes = total_params * bytes_per_param\n",
    "        size_gb = size_bytes / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"loaded\",\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"dtype\": model_dtype,\n",
    "            \"precision_name\": \"fp16 (half)\" if model_dtype == torch.float16 else \"fp32 (float)\",\n",
    "            \"bytes_per_param\": bytes_per_param,\n",
    "            \"size_gb\": size_gb,\n",
    "            \"size_formatted\": f\"{size_gb:.2f}GB\",\n",
    "            \"params_formatted\": f\"{total_params/1e9:.1f}B parameters\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Model loading logic - FAIL if no model found\n",
    "if not has_local_model:\n",
    "    raise FileNotFoundError(f\"‚ùå Model not found at: {config['model_path']}\")\n",
    "\n",
    "print(\"\\nüöÄ MODEL LOADING:\")\n",
    "print(\"   - Loading Llama-3.2-11B-Vision from local path\")\n",
    "print(f\"   - Using {device_type.upper()} for inference\")\n",
    "print(\"   - Model requires significant memory (11B parameters)\")\n",
    "print(\"   - Using fp16 precision with balanced GPU splitting\")\n",
    "\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "print(\"‚è≥ Loading Llama-3.2-11B-Vision model...\")\n",
    "\n",
    "if device_type == \"cuda\":\n",
    "    model_dtype = torch.float16  # fp16 for GPU\n",
    "    print(f\"   üîß Requesting dtype: {model_dtype} (fp16 - GPU optimized)\")\n",
    "    \n",
    "    if num_devices > 1:\n",
    "        print(f\"   üîß Using balanced GPU splitting across {num_devices} GPUs\")\n",
    "        \n",
    "        # Get available memory per GPU (reserve 4GB for safety)\n",
    "        gpu_memory = {}\n",
    "        for i in range(num_devices):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "            available_memory = total_memory - (4 * 1024**3)  # Reserve 4GB\n",
    "            gpu_memory[i] = f\"{available_memory // (1024**3)}GB\"\n",
    "        \n",
    "        print(f\"   üíæ Available memory per GPU: {gpu_memory}\")\n",
    "        \n",
    "        # Load model with balanced device map directly\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            config['model_path'],\n",
    "            torch_dtype=model_dtype,\n",
    "            device_map=\"balanced\",  # Use balanced instead of auto\n",
    "            max_memory=gpu_memory\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Model split across {num_devices} GPUs with balanced memory usage\")\n",
    "        \n",
    "    else:\n",
    "        device_map = \"cuda:0\"\n",
    "        print(f\"   üéØ Device map: {device_map}\")\n",
    "        \n",
    "        # Load model on single GPU\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            config['model_path'],\n",
    "            torch_dtype=model_dtype,\n",
    "            device_map=device_map\n",
    "        )\n",
    "        \n",
    "else:\n",
    "    model_dtype = torch.float32  # Keep fp32 for CPU compatibility\n",
    "    print(f\"   üîß Requesting dtype: {model_dtype} (fp32 - CPU compatible)\")\n",
    "    device_map = \"cpu\"\n",
    "    print(f\"   üéØ Device map: {device_map}\")\n",
    "    \n",
    "    # Load model on CPU\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        config['model_path'],\n",
    "        torch_dtype=model_dtype,\n",
    "        device_map=device_map\n",
    "    )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(config['model_path'])\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": config.get('max_tokens', 1024),\n",
    "    \"do_sample\": config.get('do_sample', False),\n",
    "    \"temperature\": config.get('temperature', 0.1)\n",
    "}\n",
    "\n",
    "# Get model size information\n",
    "model_info = get_model_size_info(model)\n",
    "\n",
    "print(\"‚úÖ Llama-3.2-11B-Vision model loaded successfully!\")\n",
    "print(f\"   üì± Device: {model.device if hasattr(model, 'device') else 'Multiple devices'}\")\n",
    "print(f\"   üéØ Actual dtype: {model.dtype}\")\n",
    "\n",
    "# Display model size information\n",
    "if model_info[\"status\"] == \"loaded\":\n",
    "    print(f\"   üî¢ Precision: {model_info['precision_name']} ({model_info['bytes_per_param']} bytes/param)\")\n",
    "    print(f\"   üìè Model size: {model_info['size_formatted']} ({model_info['params_formatted']})\")\n",
    "    print(f\"   üî¢ Total parameters: {model_info['total_params']:,}\")\n",
    "    print(f\"   üéØ Trainable parameters: {model_info['trainable_params']:,}\")\n",
    "\n",
    "# Display memory usage per GPU\n",
    "if device_type == \"cuda\":\n",
    "    print(f\"   üß† Memory usage per GPU AFTER loading:\")\n",
    "    for i in range(num_devices):\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        print(f\"      GPU {i}: {memory_allocated:.1f}GB\")\n",
    "    \n",
    "    if num_devices > 1:\n",
    "        total_memory = sum(torch.cuda.memory_allocated(i) / 1e9 for i in range(num_devices))\n",
    "        print(f\"   üìä Total memory used: {total_memory:.1f}GB across {num_devices} GPUs\")\n",
    "        print(f\"   ‚ö° Note: Model split for balanced memory usage\")\n",
    "    else:\n",
    "        print(f\"   ‚ö° Note: GPU inference will be much faster\")\n",
    "else:\n",
    "    print(f\"   üß† Memory: Managed by CPU\")\n",
    "    print(f\"   ‚ö†Ô∏è  Note: CPU inference will be slower than GPU\")\n",
    "\n",
    "print(f\"\\nüìä Configuration Summary:\")\n",
    "for key, value in config.items():\n",
    "    if isinstance(value, (str, int, float, bool)):\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Package configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPU 0 memory: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\")\n",
    "print(f\"GPU 1 memory: {torch.cuda.memory_allocated(1) / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment verification (following InternVL pattern)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"üîß ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def verify_llama_environment():\n",
    "    \"\"\"Verify Llama environment setup.\"\"\"\n",
    "    checks = {\n",
    "        \"Base path exists\": Path(config['base_path']).exists(),\n",
    "        \"Model path exists\": Path(config['model_path']).exists(),\n",
    "        \"Image folder exists\": Path(config['image_folder_path']).exists(),\n",
    "        \"Config file exists\": Path(config['config_path']).exists(),\n",
    "        \"PyTorch available\": torch is not None,\n",
    "        \"CUDA available\": torch.cuda.is_available(),\n",
    "        \"MPS available\": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Environment Check Results:\")\n",
    "    for check, result in checks.items():\n",
    "        status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "        print(f\"   {status} {check}\")\n",
    "    \n",
    "    # Memory check\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"   üìä GPU Memory: {total_memory:.1f}GB\")\n",
    "        if total_memory < 20:\n",
    "            print(\"   ‚ö†Ô∏è  Warning: Llama-3.2-11B requires 22GB+ VRAM\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"   üìä MPS Memory: Managed by macOS\")\n",
    "        print(\"   ‚ö†Ô∏è  Note: Llama-3.2-11B requires significant unified memory\")\n",
    "    \n",
    "    # Check model files\n",
    "    model_path = Path(config['model_path'])\n",
    "    if model_path.exists():\n",
    "        model_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"*.bin\"))\n",
    "        config_files = list(model_path.glob(\"config.json\"))\n",
    "        tokenizer_files = list(model_path.glob(\"tokenizer*\"))\n",
    "        \n",
    "        print(f\"   üìÅ Model files: {len(model_files)} found\")\n",
    "        print(f\"   üìÅ Config files: {len(config_files)} found\")\n",
    "        print(f\"   üìÅ Tokenizer files: {len(tokenizer_files)} found\")\n",
    "        \n",
    "        # Check if all necessary files are present\n",
    "        essential_files = model_files and config_files and tokenizer_files\n",
    "        checks[\"Essential model files present\"] = essential_files\n",
    "        status = \"‚úÖ\" if essential_files else \"‚ùå\"\n",
    "        print(f\"   {status} Essential model files present\")\n",
    "    \n",
    "    return all(checks.values())\n",
    "\n",
    "print(\"üöÄ REAL MODEL: Full environment verification...\")\n",
    "env_ok = verify_llama_environment()\n",
    "print(f\"   Environment status: {'‚úÖ Ready for inference' if env_ok else '‚ùå Issues found'}\")\n",
    "\n",
    "if env_ok and 'model' in locals():\n",
    "    print(\"   üéØ Model loaded and ready for inference\")\n",
    "    print(f\"   üì± Running on: {device_type.upper()}\")\n",
    "elif env_ok:\n",
    "    print(\"   ‚ö†Ô∏è  Model files found but not loaded (check logs above)\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment verification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Discovery and Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image discovery (following InternVL pattern)\n",
    "def discover_images() -> Dict[str, List[Path]]:\n",
    "    \"\"\"Discover images in datasets directory.\"\"\"\n",
    "    base_path = Path(config['base_path'])\n",
    "    \n",
    "    image_collections = {\n",
    "        \"test_images\": list((base_path / \"datasets/test_images\").glob(\"*.png\")) + \n",
    "                      list((base_path / \"datasets/test_images\").glob(\"*.jpg\")),\n",
    "        \"synthetic_receipts\": list((base_path / \"datasets/synthetic_receipts/images\").glob(\"*.png\")),\n",
    "        \"synthetic_bank_statements\": list((base_path / \"datasets/synthetic_bank_statements\").glob(\"*.png\")),\n",
    "    }\n",
    "    \n",
    "    # Filter existing files\n",
    "    available_images = {}\n",
    "    for category, paths in image_collections.items():\n",
    "        available_images[category] = [p for p in paths if p.exists()]\n",
    "    \n",
    "    return available_images\n",
    "\n",
    "print(\"üìÅ IMAGE DISCOVERY\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "try:\n",
    "    available_images = discover_images()\n",
    "    all_images = [img for imgs in available_images.values() for img in imgs]\n",
    "    \n",
    "    print(f\"üìä Discovery Results:\")\n",
    "    for category, images in available_images.items():\n",
    "        print(f\"   {category.replace('_', ' ').title()}: {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"      Sample: {', '.join([img.name for img in images[:2]])}\")\n",
    "    \n",
    "    print(f\"   Total: {len(all_images)} images available\")\n",
    "    \n",
    "    if all_images:\n",
    "        print(f\"\\nüéØ Sample images: {[img.name for img in all_images[:3]]}\")\n",
    "    else:\n",
    "        print(\"‚ùå No images found!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Image discovery error: {e}\")\n",
    "    available_images = {}\n",
    "    all_images = []\n",
    "\n",
    "print(\"\\n‚úÖ Image discovery completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Classification (InternVL Architecture Pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document classification using Llama model (following InternVL architecture)\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "class DocumentType(Enum):\n",
    "    \"\"\"Document types for classification.\"\"\"\n",
    "    RECEIPT = \"receipt\"\n",
    "    INVOICE = \"invoice\"\n",
    "    BANK_STATEMENT = \"bank_statement\"\n",
    "    FUEL_RECEIPT = \"fuel_receipt\"\n",
    "    TAX_INVOICE = \"tax_invoice\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    \"\"\"Result of document classification.\"\"\"\n",
    "    document_type: DocumentType\n",
    "    confidence: float\n",
    "    classification_reasoning: str\n",
    "    is_definitive: bool\n",
    "    \n",
    "    @property\n",
    "    def is_business_document(self) -> bool:\n",
    "        \"\"\"Check if document is suitable for business expense claims.\"\"\"\n",
    "        business_types = {DocumentType.RECEIPT, DocumentType.INVOICE, \n",
    "                         DocumentType.FUEL_RECEIPT, DocumentType.TAX_INVOICE}\n",
    "        return self.document_type in business_types and self.confidence > 0.8\n",
    "\n",
    "def classify_document_with_llama(image_path: str, model, processor) -> ClassificationResult:\n",
    "    \"\"\"Classify document type using Llama model.\"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Classification prompt\n",
    "    prompt = \"\"\"\n",
    "    Analyze this document image and classify it as one of:\n",
    "    - receipt: Store/business receipt\n",
    "    - invoice: Tax invoice or business invoice\n",
    "    - bank_statement: Bank account statement\n",
    "    - fuel_receipt: Petrol/fuel station receipt\n",
    "    - tax_invoice: Official tax invoice with ABN\n",
    "    - unknown: Cannot determine or not a business document\n",
    "    \n",
    "    Respond with just the classification and confidence (0-1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare inputs\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process with Llama\n",
    "    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(image, input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to same device as model\n",
    "    if hasattr(model, 'device'):\n",
    "        inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    if input_text in response:\n",
    "        response = response.split(input_text)[-1].strip()\n",
    "    \n",
    "    # Parse response to determine document type and confidence\n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    if \"receipt\" in response_lower:\n",
    "        doc_type = DocumentType.RECEIPT\n",
    "        confidence = 0.85\n",
    "    elif \"invoice\" in response_lower:\n",
    "        doc_type = DocumentType.INVOICE\n",
    "        confidence = 0.80\n",
    "    elif \"bank\" in response_lower:\n",
    "        doc_type = DocumentType.BANK_STATEMENT\n",
    "        confidence = 0.75\n",
    "    else:\n",
    "        doc_type = DocumentType.UNKNOWN\n",
    "        confidence = 0.50\n",
    "    \n",
    "    return ClassificationResult(\n",
    "        document_type=doc_type,\n",
    "        confidence=confidence,\n",
    "        classification_reasoning=f\"Llama model classification: {response[:100]}\",\n",
    "        is_definitive=confidence > 0.7\n",
    "    )\n",
    "\n",
    "print(\"üìã DOCUMENT CLASSIFICATION TEST\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"üöÄ REAL MODEL: Running document classification with Llama...\")\n",
    "\n",
    "# Test classification on first 3 images\n",
    "for i, image_path in enumerate(all_images[:3], 1):\n",
    "    print(f\"\\n{i}. Classifying: {image_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = classify_document_with_llama(\n",
    "            str(image_path), model, processor\n",
    "        )\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"   ‚è±Ô∏è  Time: {inference_time:.2f}s\")\n",
    "        print(f\"   üìÇ Type: {result.document_type.value}\")\n",
    "        print(f\"   üîç Confidence: {result.confidence:.2f}\")\n",
    "        print(f\"   üíº Business document: {'Yes' if result.is_business_document else 'No'}\")\n",
    "        print(f\"   üí≠ Reasoning: {result.classification_reasoning[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Document classification test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration Loading (Australian Tax Compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama NER configuration (preserving existing domain expertise)\n",
    "import yaml\n",
    "\n",
    "def load_ner_config() -> Dict[str, Any]:\n",
    "    \"\"\"Load NER configuration with entity definitions.\"\"\"\n",
    "    try:\n",
    "        with open(config['config_path'], 'r') as f:\n",
    "            ner_config = yaml.safe_load(f)\n",
    "        return ner_config\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Config loading failed: {e}\")\n",
    "        # Return minimal config for testing\n",
    "        return {\n",
    "            \"model\": {\n",
    "                \"name\": \"Llama-3.2-11B-Vision\",\n",
    "                \"device\": \"auto\"\n",
    "            },\n",
    "            \"entities\": {\n",
    "                \"TOTAL_AMOUNT\": {\"description\": \"Total amount including tax\"},\n",
    "                \"VENDOR_NAME\": {\"description\": \"Business/vendor name\"},\n",
    "                \"DATE\": {\"description\": \"Transaction date\"},\n",
    "                \"ABN\": {\"description\": \"Australian Business Number\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚öôÔ∏è  NER CONFIGURATION LOADING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "ner_config = load_ner_config()\n",
    "\n",
    "if 'entities' in ner_config:\n",
    "    entities = ner_config['entities']\n",
    "    print(f\"‚úÖ Loaded {len(entities)} entity types\")\n",
    "    \n",
    "    # Show key Australian compliance entities\n",
    "    australian_entities = []\n",
    "    business_entities = []\n",
    "    financial_entities = []\n",
    "    \n",
    "    for entity_name, entity_info in entities.items():\n",
    "        if any(term in entity_name for term in ['ABN', 'GST', 'BSB']):\n",
    "            australian_entities.append(entity_name)\n",
    "        elif any(term in entity_name for term in ['BUSINESS', 'VENDOR', 'COMPANY']):\n",
    "            business_entities.append(entity_name)\n",
    "        elif any(term in entity_name for term in ['AMOUNT', 'TAX', 'TOTAL', 'PRICE']):\n",
    "            financial_entities.append(entity_name)\n",
    "    \n",
    "    print(f\"\\nüá¶üá∫ Australian compliance entities ({len(australian_entities)}):\")\n",
    "    for entity in australian_entities[:5]:\n",
    "        print(f\"   - {entity}\")\n",
    "    \n",
    "    print(f\"\\nüíº Business entities ({len(business_entities)}):\")\n",
    "    for entity in business_entities[:5]:\n",
    "        print(f\"   - {entity}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Financial entities ({len(financial_entities)}):\")\n",
    "    for entity in financial_entities[:5]:\n",
    "        print(f\"   - {entity}\")\n",
    "    \n",
    "    print(f\"\\nüìä Total entities available: {len(entities)}\")\n",
    "else:\n",
    "    print(\"‚ùå No entities configuration found\")\n",
    "    entities = {}\n",
    "\n",
    "print(\"\\n‚úÖ NER configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KEY-VALUE Extraction (Primary Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY-VALUE extraction using Llama model (following InternVL pattern)\n",
    "def extract_key_value_with_llama(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced KEY-VALUE extraction for Llama responses.\"\"\"\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'extracted_data': {},\n",
    "        'confidence_score': 0.0,\n",
    "        'quality_grade': 'F',\n",
    "        'errors': [],\n",
    "        'expense_claim_format': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Parse KEY-VALUE pairs\n",
    "        extracted = {}\n",
    "        for line in response.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if ':' in line and not line.startswith('#'):\n",
    "                key, value = line.split(':', 1)\n",
    "                extracted[key.strip()] = value.strip()\n",
    "        \n",
    "        # Validate and score\n",
    "        required_fields = ['DATE', 'STORE', 'TOTAL', 'TAX']\n",
    "        found_fields = sum(1 for field in required_fields if field in extracted)\n",
    "        confidence = found_fields / len(required_fields)\n",
    "        \n",
    "        # Quality grading\n",
    "        if confidence >= 0.9:\n",
    "            grade = 'A'\n",
    "        elif confidence >= 0.7:\n",
    "            grade = 'B'\n",
    "        elif confidence >= 0.5:\n",
    "            grade = 'C'\n",
    "        else:\n",
    "            grade = 'F'\n",
    "        \n",
    "        # Convert to expense claim format\n",
    "        expense_format = {\n",
    "            'supplier_name': extracted.get('STORE', extracted.get('VENDOR', 'Unknown')),\n",
    "            'total_amount': extracted.get('TOTAL', '0.00'),\n",
    "            'transaction_date': extracted.get('DATE', ''),\n",
    "            'tax_amount': extracted.get('TAX', '0.00'),\n",
    "            'abn': extracted.get('ABN', ''),\n",
    "            'document_type': 'receipt'\n",
    "        }\n",
    "        \n",
    "        result.update({\n",
    "            'success': True,\n",
    "            'extracted_data': extracted,\n",
    "            'confidence_score': confidence,\n",
    "            'quality_grade': grade,\n",
    "            'expense_claim_format': expense_format\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['errors'].append(str(e))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_llama_prediction(image_path: str, model, processor, prompt: str) -> str:\n",
    "    \"\"\"Get prediction from Llama model.\"\"\"\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    \n",
    "    # Load image\n",
    "    if image_path.startswith('http'):\n",
    "        image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process with Llama\n",
    "    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        input_text,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to same device as model\n",
    "    if hasattr(model, 'device'):\n",
    "        inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            **generation_config,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part (after the prompt)\n",
    "    if input_text in response:\n",
    "        response = response.split(input_text)[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"üîë KEY-VALUE EXTRACTION TEST (PREFERRED METHOD)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create KEY-VALUE extraction prompt\n",
    "key_value_prompt = \"\"\"\n",
    "Extract key information from this receipt/invoice image in KEY-VALUE format.\n",
    "Use these exact keys:\n",
    "DATE: Transaction date (DD/MM/YYYY)\n",
    "STORE: Business/store name\n",
    "ABN: Australian Business Number (if present)\n",
    "TAX: Tax amount (GST)\n",
    "TOTAL: Total amount including tax\n",
    "PRODUCTS: List of items purchased\n",
    "PAYMENT_METHOD: Payment method used\n",
    "\n",
    "Format each line as KEY: VALUE\n",
    "Only extract information that is clearly visible.\n",
    "\"\"\"\n",
    "\n",
    "# Find receipt images for testing\n",
    "receipt_images = []\n",
    "for img in all_images:\n",
    "    if any(keyword in img.name.lower() for keyword in [\"receipt\", \"invoice\", \"bank\"]):\n",
    "        receipt_images.append(img)\n",
    "\n",
    "print(f\"üìÑ Found {len(receipt_images)} receipt/invoice images for testing\")\n",
    "\n",
    "print(\"üöÄ REAL MODEL: Running Key-Value extraction with Llama...\")\n",
    "\n",
    "# Test on actual receipt images\n",
    "for i, image_path in enumerate(receipt_images[:3], 1):\n",
    "    print(f\"\\n{i}. Processing: {image_path.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get model prediction\n",
    "        start_time = time.time()\n",
    "        response = get_llama_prediction(\n",
    "            str(image_path), model, processor, key_value_prompt\n",
    "        )\n",
    "        \n",
    "        # Extract with Key-Value parser\n",
    "        extraction_result = extract_key_value_with_llama(response)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"   ‚è±Ô∏è  Inference time: {inference_time:.2f}s\")\n",
    "        \n",
    "        # Show raw response (first 200 chars)\n",
    "        print(f\"   üìù Raw response: {response[:200]}...\")\n",
    "        \n",
    "        if extraction_result['success']:\n",
    "            print(f\"   ‚úÖ Extraction Success\")\n",
    "            print(f\"   üìä Confidence: {extraction_result['confidence_score']:.2f}\")\n",
    "            print(f\"   üèÜ Quality: {extraction_result['quality_grade']}\")\n",
    "            \n",
    "            # Show extracted data\n",
    "            expense_data = extraction_result['expense_claim_format']\n",
    "            print(f\"   üíº Supplier: {expense_data.get('supplier_name', 'N/A')}\")\n",
    "            print(f\"   üí∞ Amount: ${expense_data.get('total_amount', 'N/A')}\")\n",
    "            print(f\"   üìÖ Date: {expense_data.get('transaction_date', 'N/A')}\")\n",
    "            print(f\"   üá¶üá∫ ABN: {expense_data.get('abn', 'Not provided')}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ùå Extraction failed: {extraction_result.get('errors')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Key-Value extraction test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Australian Tax Compliance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Australian tax compliance validation (preserving domain expertise)\n",
    "import re\n",
    "\n",
    "def validate_australian_compliance(extracted_data: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate Australian tax compliance requirements.\"\"\"\n",
    "    compliance_result = {\n",
    "        'is_compliant': False,\n",
    "        'compliance_score': 0.0,\n",
    "        'checks': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    checks = {}\n",
    "    \n",
    "    # ABN validation\n",
    "    abn = extracted_data.get('ABN', '').replace(' ', '')\n",
    "    abn_pattern = r'^\\d{11}$'\n",
    "    checks['valid_abn'] = bool(re.match(abn_pattern, abn)) if abn else False\n",
    "    \n",
    "    # GST validation (10% in Australia)\n",
    "    try:\n",
    "        total = float(extracted_data.get('TOTAL', '0').replace('$', '').replace(',', ''))\n",
    "        tax = float(extracted_data.get('TAX', '0').replace('$', '').replace(',', ''))\n",
    "        if total > 0:\n",
    "            gst_rate = (tax / (total - tax)) * 100\n",
    "            checks['valid_gst_rate'] = abs(gst_rate - 10.0) < 1.0  # 10% ¬± 1%\n",
    "        else:\n",
    "            checks['valid_gst_rate'] = False\n",
    "    except:\n",
    "        checks['valid_gst_rate'] = False\n",
    "    \n",
    "    # Date format validation (Australian DD/MM/YYYY)\n",
    "    date = extracted_data.get('DATE', '')\n",
    "    aus_date_pattern = r'^\\d{2}/\\d{2}/\\d{4}$'\n",
    "    checks['valid_date_format'] = bool(re.match(aus_date_pattern, date))\n",
    "    \n",
    "    # Business name validation\n",
    "    business_name = extracted_data.get('STORE', extracted_data.get('VENDOR', ''))\n",
    "    checks['has_business_name'] = len(business_name.strip()) > 0\n",
    "    \n",
    "    # Total amount validation\n",
    "    checks['has_total_amount'] = total > 0 if 'total' in locals() else False\n",
    "    \n",
    "    # Calculate compliance score\n",
    "    score = sum(checks.values()) / len(checks)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    if not checks['valid_abn']:\n",
    "        recommendations.append(\"ABN should be 11 digits for Australian businesses\")\n",
    "    if not checks['valid_gst_rate']:\n",
    "        recommendations.append(\"GST rate should be 10% for Australian transactions\")\n",
    "    if not checks['valid_date_format']:\n",
    "        recommendations.append(\"Date should be in DD/MM/YYYY format\")\n",
    "    \n",
    "    compliance_result.update({\n",
    "        'is_compliant': score >= 0.8,\n",
    "        'compliance_score': score,\n",
    "        'checks': checks,\n",
    "        'recommendations': recommendations\n",
    "    })\n",
    "    \n",
    "    return compliance_result\n",
    "\n",
    "print(\"üá¶üá∫ AUSTRALIAN TAX COMPLIANCE VALIDATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test compliance validation with sample data\n",
    "sample_extractions = [\n",
    "    {\n",
    "        'STORE': 'WOOLWORTHS SUPERMARKET',\n",
    "        'ABN': '88 000 014 675',\n",
    "        'DATE': '08/06/2024',\n",
    "        'TOTAL': '42.08',\n",
    "        'TAX': '3.83'\n",
    "    },\n",
    "    {\n",
    "        'STORE': 'BUNNINGS WAREHOUSE',\n",
    "        'ABN': '12345678901',  # Invalid format\n",
    "        'DATE': '2024-06-08',  # Wrong format\n",
    "        'TOTAL': '156.90',\n",
    "        'TAX': '14.26'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, extraction in enumerate(sample_extractions, 1):\n",
    "    print(f\"\\n{i}. Testing: {extraction['STORE']}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    compliance = validate_australian_compliance(extraction)\n",
    "    \n",
    "    print(f\"   üìä Compliance Score: {compliance['compliance_score']:.2f}\")\n",
    "    print(f\"   ‚úÖ Is Compliant: {'Yes' if compliance['is_compliant'] else 'No'}\")\n",
    "    \n",
    "    print(f\"   üîç Detailed Checks:\")\n",
    "    for check, result in compliance['checks'].items():\n",
    "        status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "        print(f\"      {status} {check.replace('_', ' ').title()}\")\n",
    "    \n",
    "    if compliance['recommendations']:\n",
    "        print(f\"   üí° Recommendations:\")\n",
    "        for rec in compliance['recommendations']:\n",
    "            print(f\"      - {rec}\")\n",
    "\n",
    "print(f\"\\nüèÜ COMPLIANCE FEATURES:\")\n",
    "print(f\"   ‚úÖ ABN validation (11-digit Australian Business Number)\")\n",
    "print(f\"   ‚úÖ GST rate validation (10% Australian standard)\")\n",
    "print(f\"   ‚úÖ Date format validation (DD/MM/YYYY Australian format)\")\n",
    "print(f\"   ‚úÖ Business name extraction and validation\")\n",
    "print(f\"   ‚úÖ Total amount validation and calculation\")\n",
    "\n",
    "print(\"\\n‚úÖ Australian tax compliance validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CLI Interface Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI interface demonstration (following InternVL pattern)\n",
    "print(\"üñ•Ô∏è  CLI INTERFACE INTEGRATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"üìã Available CLI Commands:\")\n",
    "print(\"\\nüîß Using current tax_invoice_ner CLI:\")\n",
    "if is_local:\n",
    "    print(\"   uv run python -m tax_invoice_ner.cli extract <image_path>\")\n",
    "    print(\"   uv run python -m tax_invoice_ner.cli list-entities\")\n",
    "    print(\"   uv run python -m tax_invoice_ner.cli validate-config\")\n",
    "else:\n",
    "    print(\"   python -m tax_invoice_ner.cli extract <image_path>\")\n",
    "    print(\"   python -m tax_invoice_ner.cli list-entities\")\n",
    "    print(\"   python -m tax_invoice_ner.cli validate-config\")\n",
    "\n",
    "print(\"\\nüéØ Enhanced CLI (following InternVL architecture):\")\n",
    "future_commands = [\n",
    "    \"single_extract.py - Single document processing with auto-classification\",\n",
    "    \"batch_extract.py - Batch processing with parallel execution\",\n",
    "    \"classify.py - Document type classification only\",\n",
    "    \"evaluate.py - SROIE-compatible evaluation pipeline\"\n",
    "]\n",
    "\n",
    "for cmd in future_commands:\n",
    "    name, desc = cmd.split(' - ')\n",
    "    print(f\"   üìÑ {name} - {desc}\")\n",
    "\n",
    "print(\"\\nüî¨ Working Examples with Current CLI:\")\n",
    "test_images_path = config['image_folder_path']\n",
    "\n",
    "sample_commands = [\n",
    "    f\"extract {test_images_path}/invoice.png\",\n",
    "    f\"extract {test_images_path}/bank_statement_sample.png\",\n",
    "    f\"extract {test_images_path}/test_receipt.png --entities TOTAL_AMOUNT VENDOR_NAME DATE\"\n",
    "]\n",
    "\n",
    "for i, cmd in enumerate(sample_commands, 1):\n",
    "    if is_local:\n",
    "        full_cmd = f\"uv run python -m tax_invoice_ner.cli {cmd}\"\n",
    "    else:\n",
    "        full_cmd = f\"python -m tax_invoice_ner.cli {cmd}\"\n",
    "    print(f\"   {i}. {full_cmd}\")\n",
    "\n",
    "print(\"\\nüìä Enhanced Features (InternVL Architecture):\")\n",
    "enhanced_features = [\n",
    "    \"Environment-driven configuration (.env files)\",\n",
    "    \"Automatic document classification with confidence scoring\",\n",
    "    \"KEY-VALUE extraction (preferred over JSON)\",\n",
    "    \"Australian tax compliance validation\",\n",
    "    \"Batch processing with parallel execution\",\n",
    "    \"SROIE-compatible evaluation pipeline\",\n",
    "    \"Cross-platform deployment (local Mac ‚Üî remote GPU)\"\n",
    "]\n",
    "\n",
    "for feature in enhanced_features:\n",
    "    print(f\"   ‚úÖ {feature}\")\n",
    "\n",
    "print(\"\\nüí° Migration Benefits:\")\n",
    "benefits = [\n",
    "    \"Retain proven Llama-3.2-11B-Vision model quality\",\n",
    "    \"Adopt InternVL's superior modular architecture\",\n",
    "    \"Preserve Australian tax compliance features\",\n",
    "    \"Enhance deployment flexibility and maintainability\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(f\"   üéØ {benefit}\")\n",
    "\n",
    "print(\"\\n‚úÖ CLI interface integration documented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison (Llama vs InternVL architecture)\n",
    "print(\"üìä PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Performance metrics comparison\n",
    "performance_comparison = {\n",
    "    \"Model Size\": {\n",
    "        \"Llama-3.2-11B-Vision\": \"11B parameters\",\n",
    "        \"InternVL3-8B\": \"8B parameters\"\n",
    "    },\n",
    "    \"Memory Requirements\": {\n",
    "        \"Llama-3.2-11B-Vision\": \"22GB+ VRAM\",\n",
    "        \"InternVL3-8B\": \"~4GB VRAM\"\n",
    "    },\n",
    "    \"Mac M1 Compatibility\": {\n",
    "        \"Llama-3.2-11B-Vision\": \"Limited (memory constraints)\",\n",
    "        \"InternVL3-8B\": \"Full MPS support\"\n",
    "    },\n",
    "    \"Document Specialization\": {\n",
    "        \"Llama-3.2-11B-Vision\": \"General vision + strong language\",\n",
    "        \"InternVL3-8B\": \"Document-focused training\"\n",
    "    },\n",
    "    \"Australian Tax Features\": {\n",
    "        \"Llama-3.2-11B-Vision\": \"Comprehensive (35+ entities)\",\n",
    "        \"InternVL3-8B\": \"Basic (needs enhancement)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîç Detailed Comparison:\")\n",
    "for metric, comparison in performance_comparison.items():\n",
    "    print(f\"\\nüìã {metric}:\")\n",
    "    for model, value in comparison.items():\n",
    "        print(f\"   ‚Ä¢ {model}: {value}\")\n",
    "\n",
    "print(\"\\nüéØ HYBRID APPROACH BENEFITS:\")\n",
    "hybrid_benefits = [\n",
    "    \"‚úÖ Retain Llama's superior entity recognition quality\",\n",
    "    \"‚úÖ Adopt InternVL's modular architecture patterns\",\n",
    "    \"‚úÖ Keep comprehensive Australian compliance features\",\n",
    "    \"‚úÖ Improve deployment flexibility and maintainability\",\n",
    "    \"‚úÖ Environment-driven configuration for cross-platform deployment\",\n",
    "    \"‚úÖ KEY-VALUE extraction for better reliability\",\n",
    "    \"‚úÖ Automatic document classification with confidence scoring\"\n",
    "]\n",
    "\n",
    "for benefit in hybrid_benefits:\n",
    "    print(f\"   {benefit}\")\n",
    "\n",
    "print(\"\\nüìà Expected Improvements:\")\n",
    "improvements = {\n",
    "    \"Architecture\": \"20-30% better maintainability\",\n",
    "    \"Deployment\": \"Cross-platform compatibility\",\n",
    "    \"Extraction Reliability\": \"KEY-VALUE vs JSON parsing\",\n",
    "    \"Configuration Management\": \"Environment-driven (.env files)\",\n",
    "    \"Testing Framework\": \"SROIE-compatible evaluation\"\n",
    "}\n",
    "\n",
    "for area, improvement in improvements.items():\n",
    "    print(f\"   üìä {area}: {improvement}\")\n",
    "\n",
    "print(\"\\nüèÜ RECOMMENDED APPROACH:\")\n",
    "print(\"   üéØ Use Llama-3.2-11B-Vision model (proven quality)\")\n",
    "print(\"   üèóÔ∏è  Adopt InternVL PoC architecture (superior design)\")\n",
    "print(\"   üá¶üá∫ Preserve Australian tax compliance (domain expertise)\")\n",
    "print(\"   üöÄ Best of both worlds: Quality + Architecture\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance comparison completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Package Summary and Migration Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package testing summary and migration roadmap\n",
    "print(\"üéØ LLAMA 3.2-11B VISION NER PACKAGE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüì¶ Package Modules Tested (InternVL Architecture Pattern):\")\n",
    "modules_tested = [\n",
    "    \"Local Llama-3.2-11B-Vision model loading\",\n",
    "    \"Environment-driven configuration (.env files)\",\n",
    "    \"Automatic device detection and MPS optimization\",\n",
    "    \"Document classification with confidence scoring\",\n",
    "    \"KEY-VALUE extraction (preferred over JSON)\",\n",
    "    \"Australian tax compliance validation\",\n",
    "    \"Performance metrics and evaluation\",\n",
    "    \"Cross-platform deployment support\"\n",
    "]\n",
    "\n",
    "for module in modules_tested:\n",
    "    print(f\"   ‚úÖ {module}\")\n",
    "\n",
    "print(\"\\nüîë Key Features Demonstrated:\")\n",
    "key_features = [\n",
    "    \"Real Llama-3.2-11B-Vision model integration from local path\",\n",
    "    \"MPS acceleration for Mac M1 compatibility\",\n",
    "    \"Modular architecture (following InternVL pattern)\",\n",
    "    \"Australian business compliance (ABN, GST, date formats)\",\n",
    "    \"KEY-VALUE extraction with quality grading\",\n",
    "    \"Document classification for business documents\",\n",
    "    \"Environment-based configuration management\"\n",
    "]\n",
    "\n",
    "for feature in key_features:\n",
    "    print(f\"   üéØ {feature}\")\n",
    "\n",
    "print(f\"\\nüìä Environment Status:\")\n",
    "model_status = \"Loaded from local path\" if has_local_model and not isinstance(model, str) else \"Mock objects (model not found/loaded)\"\n",
    "inference_status = \"Full functionality available\" if has_local_model and not isinstance(model, str) else \"Mock mode - load actual model for inference\"\n",
    "\n",
    "print(f\"   üñ•Ô∏è  Environment: {'Mac M1 with MPS' if is_local else 'Remote GPU'}\")\n",
    "print(f\"   üìÇ Model path: {config['model_path']}\")\n",
    "print(f\"   üîç Local model: {'‚úÖ Found' if has_local_model else '‚ùå Not found'}\")\n",
    "print(f\"   ü§ñ Model: {model_status}\")\n",
    "print(f\"   üîÑ Inference: {inference_status}\")\n",
    "print(f\"   üìÅ Images: {len(all_images)} discovered\")\n",
    "print(f\"   ‚öôÔ∏è  Entities: {len(entities)} configured\")\n",
    "\n",
    "print(\"\\nüöÄ MIGRATION ROADMAP:\")\n",
    "print(\"\\nüìÖ Phase 1: Core Architecture (Weeks 1-2)\")\n",
    "phase1_tasks = [\n",
    "    \"Implement environment-driven configuration\",\n",
    "    \"Create modular processor architecture\",\n",
    "    \"Add automatic document classification\",\n",
    "    \"Migrate to KEY-VALUE extraction\"\n",
    "]\n",
    "\n",
    "for task in phase1_tasks:\n",
    "    print(f\"   üìã {task}\")\n",
    "\n",
    "print(\"\\nüìÖ Phase 2: Feature Enhancement (Weeks 3-4)\")\n",
    "phase2_tasks = [\n",
    "    \"Enhance CLI with batch processing\",\n",
    "    \"Implement SROIE evaluation pipeline\",\n",
    "    \"Add cross-platform deployment support\",\n",
    "    \"Create comprehensive testing framework\"\n",
    "]\n",
    "\n",
    "for task in phase2_tasks:\n",
    "    print(f\"   üìã {task}\")\n",
    "\n",
    "print(\"\\nüìÖ Phase 3: Production Readiness (Week 5)\")\n",
    "phase3_tasks = [\n",
    "    \"Performance benchmarking and optimization\",\n",
    "    \"Documentation and migration guides\",\n",
    "    \"KFP-ready containerization\",\n",
    "    \"Production deployment validation\"\n",
    "]\n",
    "\n",
    "for task in phase3_tasks:\n",
    "    print(f\"   üìã {task}\")\n",
    "\n",
    "print(\"\\nüèÜ EXPECTED OUTCOMES:\")\n",
    "outcomes = [\n",
    "    \"Production-ready system combining Llama quality + InternVL architecture\",\n",
    "    \"Enhanced maintainability and deployment flexibility\",\n",
    "    \"Preserved Australian tax compliance expertise\",\n",
    "    \"Improved extraction reliability with KEY-VALUE format\",\n",
    "    \"Local Mac M1 compatibility with MPS acceleration\"\n",
    "]\n",
    "\n",
    "for outcome in outcomes:\n",
    "    print(f\"   üéØ {outcome}\")\n",
    "\n",
    "print(\"\\nüéâ LLAMA 3.2-11B VISION NER WITH INTERNVL ARCHITECTURE READY!\")\n",
    "print(f\"   Model Quality: ‚úÖ Llama-3.2-11B-Vision from local path\")\n",
    "print(f\"   Architecture: ‚úÖ InternVL PoC modular design\")\n",
    "print(f\"   Compliance: ‚úÖ Australian tax requirements\")\n",
    "print(f\"   Local Support: ‚úÖ Mac M1 MPS acceleration\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "if has_local_model and not isinstance(model, str):\n",
    "    print(\"   1. ‚úÖ Local model loaded - run full extraction pipeline\")\n",
    "    print(\"   2. Test KEY-VALUE extraction on real images\")\n",
    "    print(\"   3. Validate extraction quality vs current system\")\n",
    "    print(\"   4. Begin Phase 1 architecture migration\")\n",
    "elif has_local_model:\n",
    "    print(\"   1. ‚ö†Ô∏è  Model files found but loading failed - check dependencies\")\n",
    "    print(\"   2. Install required packages: transformers, torch, pillow\")\n",
    "    print(\"   3. Retry model loading in conda environment\")\n",
    "    print(\"   4. Test full pipeline once model loads\")\n",
    "else:\n",
    "    print(\"   1. üì• Download Llama-3.2-11B-Vision to /Users/tod/PretrainedLLM/\")\n",
    "    print(\"   2. Ensure model files are complete (safetensors, config.json, tokenizer)\")\n",
    "    print(\"   3. Re-run notebook to load actual model\")\n",
    "    print(\"   4. Test full inference pipeline\")\n",
    "\n",
    "print(\"   5. Execute 5-week migration roadmap\")\n",
    "print(\"   6. Deploy hybrid system to production\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook configuration updated for local model loading!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}