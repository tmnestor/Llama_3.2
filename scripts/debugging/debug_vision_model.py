#!/usr/bin/env python3
"""
Debug script for Llama-3.2-Vision model integrity and vision processing.
"""

import os
from pathlib import Path

import torch
from PIL import Image
from transformers import AutoProcessor, MllamaForConditionalGeneration


def check_model_integrity(model_path: str):
    """Check model files and configuration integrity."""
    model_path = Path(model_path)

    print("ğŸ” MODEL INTEGRITY CHECK")
    print("=" * 40)

    # Check model files
    required_files = [
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
    ]

    model_files = list(model_path.glob("*.safetensors")) + list(
        model_path.glob("*.bin")
    )

    print(f"ğŸ“ Model directory: {model_path}")
    print(f"ğŸ“„ Config files: {[f.name for f in model_path.glob('*.json')]}")
    print(f"ğŸ”§ Model files: {len(model_files)} found")
    print(f"ğŸ“ Tokenizer files: {[f.name for f in model_path.glob('tokenizer*')]}")

    for file in required_files:
        if (model_path / file).exists():
            print(f"   âœ… {file}")
        else:
            print(f"   âŒ {file} - MISSING")

    return len(model_files) > 0


def test_model_loading(model_path: str):
    """Test model loading without inference."""
    print("\nğŸš€ MODEL LOADING TEST")
    print("=" * 40)

    try:
        # Load processor first
        print("ğŸ“‹ Loading processor...")
        processor = AutoProcessor.from_pretrained(
            model_path, trust_remote_code=True, local_files_only=True
        )
        print("   âœ… Processor loaded")

        # Check processor components
        print(f"   ğŸ“ Tokenizer vocab size: {len(processor.tokenizer)}")
        print(f"   ğŸ–¼ï¸  Image processor: {type(processor.image_processor).__name__}")

        # Test tokenizer on vision tokens
        test_text = "<|image|>What is in this image?"
        tokens = processor.tokenizer(test_text, return_tensors="pt")
        print(f"   ğŸ”¤ Test tokenization: {tokens['input_ids'].shape}")

        # Load model with minimal config
        print("\nğŸ¤– Loading model...")
        model = MllamaForConditionalGeneration.from_pretrained(
            model_path,
            device_map=None,  # Load to CPU first
            torch_dtype=torch.float16,
            trust_remote_code=True,
            local_files_only=True,
            low_cpu_mem_usage=True,
        )
        print("   âœ… Model loaded to CPU")

        # Check model components
        print(f"   ğŸ§  Model type: {type(model).__name__}")
        print(
            f"   ğŸ“Š Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B"
        )

        # Check vision model
        if hasattr(model, "vision_model"):
            print("   ğŸ‘ï¸  Vision model: Present")
        else:
            print("   âŒ Vision model: Missing")

        # Check language model
        if hasattr(model, "language_model"):
            print("   ğŸ’¬ Language model: Present")
        else:
            print("   âŒ Language model: Missing")

        return model, processor

    except Exception as e:
        print(f"   âŒ Error: {e}")
        return None, None


def test_cpu_inference(model, processor, image_path: str):
    """Test inference on CPU to isolate CUDA issues."""
    print("\nğŸ–¥ï¸  CPU INFERENCE TEST")
    print("=" * 40)

    if model is None or processor is None:
        print("âŒ Model/processor not available")
        return False

    try:
        # Load a simple test image
        if not Path(image_path).exists():
            print(f"âŒ Test image not found: {image_path}")
            return False

        image = Image.open(image_path).convert("RGB")
        print(f"ğŸ–¼ï¸  Image loaded: {image.size}")

        # Simple prompt
        prompt = "<|image|>What is in this image?"

        # Process on CPU
        print("ğŸ”„ Processing inputs on CPU...")
        inputs = processor(text=prompt, images=image, return_tensors="pt")

        print(f"   ğŸ“¥ Input IDs shape: {inputs['input_ids'].shape}")
        print(f"   ğŸ–¼ï¸  Pixel values shape: {inputs['pixel_values'].shape}")

        # Try CPU generation with minimal tokens
        print("ğŸ§® Testing CPU generation...")
        model.eval()

        with torch.no_grad():
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=10,  # Very short generation
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id,
                )

                response = processor.decode(
                    outputs[0][inputs["input_ids"].shape[-1] :],
                    skip_special_tokens=True,
                )

                print(f"   âœ… CPU generation successful: '{response}'")
                return True

            except Exception as e:
                print(f"   âŒ CPU generation failed: {e}")
                return False

    except Exception as e:
        print(f"âŒ CPU test failed: {e}")
        return False


def main():
    """Main diagnostic function."""
    # Use environment variable for model path
    model_path = os.getenv(
        "TAX_INVOICE_NER_MODEL_PATH",
        "/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision",
    )
    image_path = (
        os.getenv(
            "TAX_INVOICE_NER_IMAGE_PATH", "/home/jovyan/nfs_share/tod/data/examples"
        )
        + "/test_receipt.png"
    )

    print("ğŸ©º LLAMA-3.2-VISION DIAGNOSTIC")
    print("=" * 50)
    print(f"ğŸ¯ Model path: {model_path}")
    print(f"ğŸ–¼ï¸  Test image: {image_path}")

    # Step 1: Check model integrity
    if not check_model_integrity(model_path):
        print("\nâŒ Model files incomplete - cannot proceed")
        return

    # Step 2: Test model loading
    model, processor = test_model_loading(model_path)

    # Step 3: Test CPU inference
    if model and processor:
        cpu_success = test_cpu_inference(model, processor, image_path)

        if cpu_success:
            print("\nâœ… CPU inference works - CUDA issue is device-specific")
            print("ğŸ’¡ Recommendations:")
            print("   1. Try different CUDA device mapping")
            print("   2. Check GPU memory constraints")
            print("   3. Use CPU inference as fallback")
        else:
            print("\nâŒ CPU inference also fails - model checkpoint issue")
            print("ğŸ’¡ Recommendations:")
            print("   1. Re-download model checkpoint")
            print("   2. Verify transformers version compatibility")
            print("   3. Try different model variant")

    print("\nğŸ Diagnostic complete")


if __name__ == "__main__":
    main()
